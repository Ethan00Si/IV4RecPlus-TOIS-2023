lr: 0.0001
min_lr: 0.00001
weight_decay: 0.0 # l2 norm
patience: 5 # Number of epochs with no improvement after which learning rate will be reduced
es_patience: 5 #patience for early stop

input_emb_size: 768 #bert size
item_dim: 64
hid_units: [200, 80, 1]
dropout: 0.0

l2 : 0.00001

IV_NET:
  #multi-head self-att
  qry_dim: 768
  rec_item_dim: 64
  num_heads : 8
 
  # additive attention
  src_his_step: 50
  dense: 200

  dropout: 0.1
  hid_units: [128, 128, 64]

  lambda: 0.9

  item_IV_NET:
    hid_units: [64]
    dropout: 0.1

    lambda: 0.9

Agg:
  input_dim: 128
  hid_units: [1]


item_Agg:
  input_dim: 128
  hid_units: [1]
